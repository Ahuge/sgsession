
- use sgschema for resolving field names

  - schema.resolve_one() -> assert there is only one
  - schema.resolve should deal with dotted names
  
  - need to special-case "type" everywhere
  - session.create() -> resolve the keys
  - session.find() -> resolve the filters, order, return_fields
  - session.update() -> resolve the data
  - session.merge() -> resolve the data; making the assumption that the
    data is coming from Shotgun, it will not get mutated, so it is okay that
    there isn't a seperate merge for Shotgun vs user data.

  - use $sgsession:parent alias
  - use #sgsession:preload tag

  - use allowed_entity_types to do auto-loading



- mutable collections: BoundList and BoundDict
    - attach to an Entity and merge all new data into the session
    - allow native tuples, but merge everything in them
    - in the future they can mark an entity as updated so that session.commit()
      can update everything in a batch
    	- session.create(*, **, defer=True) will be created during a
    	  commit

- fetch should complain when the entity doesn't exist, or mark the entity as
  retired. Perhaps it is best to fail quickly and raise EntityIDError

- entity.refresh()
- session.refresh(entities)

- entity.commit(), an Entity-level update
- entity.delete()

- only fetch the "important" fields/links that are not already satisfied, unless
  explicitly requested to do so

- ORM-style queries, to simplify the new deep-filters
  
  sg.query('PublishEvent', 'sg_type', 'sg_version').filter('id', '>', 12).order_by('created_at').one()
  sg.query('Task', 'content', 'step').filter({'assignees': me}, 'or', {'assignees': mark}
  sg.get(type_, id) -> sg.Task(id) -> sg.query(type_).filter('id', 'is', id).first()

  
- track fields which don't exist with a sentinel so that Entity.fetch(...) knows to not go looking for it on the next call.

- with session.batch() as batch:
    x = batch.create('Task', ...)

